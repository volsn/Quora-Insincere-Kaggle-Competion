{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, recall_score, \\\n",
    "                                precision_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haven`t find better way to download encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "(train_data, test_data), info = tfds.load(\n",
    "    'imdb_reviews/subwords8k',\n",
    "    split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    with_info=True, as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('pickles', 'encoder.pickle'), 'wb') as handle:\n",
    "    pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_submission.csv',\n",
       " 'embeddings',\n",
       " 'test.csv',\n",
       " 'train.csv',\n",
       " 'sample_submission.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f for f in os.listdir('DATA') if not f.startswith('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_train(path='DATA'):\n",
    "    data = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "    data_positive = data.loc[data['target'] == 1][:80000]\n",
    "    data_negative = data.loc[data['target'] == 0][:80000]\n",
    "    \n",
    "    data = data_positive.append(data_negative, ignore_index=True, sort=False)\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X = data.drop(['qid', 'target'], axis=1).values\n",
    "    y = data['target'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def fetch_data_test(path='DATA'):\n",
    "    train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "    test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
    "    \n",
    "    X_train = train.drop(['qid', 'target'], axis=1).values\n",
    "    y_train = train['target'].values\n",
    "    X_test = train.drop(['qid'], axis=1).values\n",
    "    \n",
    "    return X_train, X_test, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = fetch_data_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"What do I do to crack IGIDR in three months' time? (MSc economics)\"],\n",
       "       ['What is the best thing you have ever encountered?'],\n",
       "       ['Which subjects should I choose for fashion technology in class 11th?'],\n",
       "       ...,\n",
       "       ['Can you show me pussy photos?'],\n",
       "       ['Should Donald Trump sue Mexico for the Chicxulub crater that caused the extinction of almost all life on the planet Earth?'],\n",
       "       ['Did Abraham use the DMT contained in the Acacia tree, to contact Yahweh?']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: What are the best ways to warm bread in the oven?\n",
      "Encoded sentence [274, 29, 1, 175, 1766, 7, 1892, 7961, 2144, 2189, 11, 1, 1928, 413, 7992]\n"
     ]
    }
   ],
   "source": [
    "example_sentence = X_train[0][0]\n",
    "encoded_sentence = encoder.encode(example_sentence)\n",
    "decoded_sentence = encoder.decode(encoded_sentence)\n",
    "\n",
    "assert example_sentence == decoded_sentence\n",
    "\n",
    "print('Original sentence:', decoded_sentence)\n",
    "print('Encoded sentence', encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom embedding pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, encoder):\n",
    "        self.encoder = encoder\n",
    "        self.vocab_size = self.encoder.vocab_size\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = list()\n",
    "        for line in X.reshape(1, -1)[0]:\n",
    "            X_transformed.append(\n",
    "                self.encoder.encode(line)\n",
    "            )\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingTransform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_len=None):\n",
    "        if max_len is not None:\n",
    "            self.max_len = max_len\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if 'max_len' not in self.__dir__():\n",
    "            self.max_len = len(max(X, key=len))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        \n",
    "        for line in X:\n",
    "            X_transformed.append(\n",
    "                np.concatenate((\n",
    "                    np.array(line),\n",
    "                    np.array([0] * (self.max_len - len(line)))\n",
    "                ), axis=0)\n",
    "            )\n",
    "        return np.array(X_transformed, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('embedding', EmbeddingTransformer(encoder=encoder)),\n",
    "    ('padding', PaddingTransform())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(np.concatenate((\n",
    "    X_train, X_test\n",
    "), axis=0))\n",
    "X_train = pipeline.transform(X_train)\n",
    "X_test = pipeline.transform(X_test)\n",
    "\n",
    "# Use part of data for validation\n",
    "X_valid = X_train[-1000:]\n",
    "y_valid = y_train[-1000:]\n",
    "X_train = X_train[:-1000]\n",
    "y_train = y_train[:-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (8000, 125)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 274,   29,    1, ...,    0,    0,    0],\n",
       "       [ 274,   29,   63, ...,    0,    0,    0],\n",
       "       [ 274,    9,   74, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 809,  110, 2640, ...,    0,    0,    0],\n",
       "       [ 809,   18,    1, ...,    0,    0,    0],\n",
       "       [ 809,    9,  417, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('X shape:', X_train.shape)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = {\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train,\n",
    "    'X_valid': X_valid,\n",
    "    'y_valid': y_valid,\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('pickles', 'dataset.pickle'), 'wb') as handle:\n",
    "    pickle.dump(to_save, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some popular solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = fetch_data_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "max_features = 50000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X_train.reshape(1, -1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train.reshape(1, -1)[0])\n",
    "X_test = tokenizer.texts_to_sequences(X_test.reshape(1, -1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     1, 11922, 18557],\n",
       "       [    0,     0,     0, ...,   167,     2,   110],\n",
       "       [    0,     0,     0, ...,   415,  1030,  1119],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  1968,    14,     1],\n",
       "       [    0,     0,     0, ...,   688,  6050,  3516],\n",
       "       [    0,     0,     0, ...,   313,  1245,  1692]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford`s Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(*['DATA', 'embeddings', 'glove.840B.300d', 'glove.840B.300d.txt'])\n",
    "def load_coefs(word, *vect):\n",
    "    return word, np.array(vect, dtype=np.float32)\n",
    "embeddings = dict(load_coefs(*w.split(\" \")) for w in open(file, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 2196016\n",
      "Embedding vector shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "print('Number of embeddings:', len(embeddings.keys()))\n",
    "print('Embedding vector shape:', embeddings['foo'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.082752,  0.67204 , -0.14987 , ..., -0.1918  , -0.37846 ,\n",
       "        -0.06589 ],\n",
       "       [ 0.012001,  0.20751 , -0.12578 , ...,  0.13871 , -0.36049 ,\n",
       "        -0.035   ],\n",
       "       [ 0.27204 , -0.06203 , -0.1884  , ...,  0.13015 , -0.18317 ,\n",
       "         0.1323  ],\n",
       "       ...,\n",
       "       [ 0.7344  , -0.33641 ,  0.26918 , ...,  0.63718 , -0.13914 ,\n",
       "        -0.16472 ],\n",
       "       [ 0.21215 , -0.99456 ,  1.1782  , ...,  0.93427 , -0.93286 ,\n",
       "        -0.51479 ],\n",
       "       [-0.07969 , -0.22905 ,  0.80366 , ..., -0.083561,  0.48532 ,\n",
       "        -0.7313  ]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_matrix = embedding_matrix.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9871725 ,  0.93450505,  0.19410798, ...,  0.0385003 ,\n",
       "         0.05576302, -0.67120665],\n",
       "       [ 0.27204   , -0.06203   , -0.1884    , ...,  0.13015   ,\n",
       "        -0.18317   ,  0.1323    ],\n",
       "       [ 0.31924   ,  0.06316   , -0.27858   , ...,  0.082745  ,\n",
       "         0.097801  ,  0.25045   ],\n",
       "       ...,\n",
       "       [ 0.6047153 ,  0.28296432, -0.23499994, ..., -0.40001133,\n",
       "        -0.24867615, -0.342037  ],\n",
       "       [ 0.915036  , -0.61523443, -0.3464703 , ...,  0.8471742 ,\n",
       "        -0.3274441 ,  0.20680809],\n",
       "       [-0.08819957, -0.3774332 , -0.00939436, ..., -0.65271074,\n",
       "         1.1646155 ,  0.20724036]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 300)         15000000  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                4816      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,004,833\n",
      "Trainable params: 15,004,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(nb_words, embed_size),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128000 samples\n",
      "128000/128000 [==============================] - 453s 4ms/sample - loss: 0.2947 - accuracy: 0.8827\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=1,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred[np.where(y_pred >= 0.5)] = 1\n",
    "y_pred[np.where(y_pred < 0.5)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8888125\n",
      "Recall: 0.8948978954599388\n",
      "Precision: 0.8842949706880593\n",
      "F1 Score: 0.8895648395306972\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('F1 Score:', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
